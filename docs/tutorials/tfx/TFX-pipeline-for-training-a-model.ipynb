{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "penguin_transform.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DjUA6S30k52h"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjUA6S30k52h"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNWyqewk8fE"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1ypzczQCwy"
      },
      "source": [
        "# Feature Engineering using TFX Pipeline and TensorFlow Transform\n",
        "\n",
        "***Transform input data and traing a model with a TFX pipeline.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9YYythm0dx"
      },
      "source": [
        "Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".\n",
        "\n",
        "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft\">\n",
        "<img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"/>View on TensorFlow.org</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_tft.ipynb\">\n",
        "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a></td>\n",
        "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/penguin_tft.ipynb\">\n",
        "<img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">View source on GitHub</a></td>\n",
        "<td><a href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/penguin_tft.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td>\n",
        "</table></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuwrlnvQJ5k"
      },
      "source": [
        "In this notebook-based tutorial, we will create and run a TFX pipeline\n",
        "to ingest raw input data and preprocess it appropriately for ML training.\n",
        "This notebook is based on the TFX pipeline we built in\n",
        "[Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv).\n",
        "If you have not read that one yet, you should read it before proceeding with\n",
        "this notebook.\n",
        "\n",
        "You can increase the predictive quality of your data and/or reduce\n",
        "dimensionality with feature engineering. One of the benefits of using TFX is\n",
        "that you will write your transformation code once, and the resulting transforms\n",
        "will be consistent between training and serving in\n",
        "order to avoid training/serving skew.\n",
        "\n",
        "We will add a `Transform` component to the pipeline. The Transform component is\n",
        "implemented using the\n",
        "[tf.transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n",
        "\n",
        "Please see\n",
        "[Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines)\n",
        "to learn more about various concepts in TFX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmgi8ZvQkScg"
      },
      "source": [
        "## Set Up\n",
        "We first need to install the TFX Python package and download\n",
        "the dataset which we will use for our model.\n",
        "\n",
        "### Upgrade Pip\n",
        "\n",
        "To avoid upgrading Pip in a system when running locally,\n",
        "check to make sure that we are running in Colab.\n",
        "Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4OTe2ukSqm",
        "outputId": "2b067659-23f8-48f1-d1a2-3d7b828cd225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### Install TFX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQtljP-qPHY",
        "outputId": "4cd3cd7c-c6c6-4aac-a09a-0ce04aad9516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -U tfx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tfx\n",
            "  Downloading tfx-1.4.0-py3-none-any.whl (2.4 MB)\n",
            "     |████████████████████████████████| 2.4 MB 4.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n",
            "Collecting ml-pipelines-sdk==1.4.0\n",
            "  Downloading ml_pipelines_sdk-1.4.0-py3-none-any.whl (1.2 MB)\n",
            "     |████████████████████████████████| 1.2 MB 70.8 MB/s            \n",
            "\u001b[?25hCollecting packaging<21,>=20\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "     |████████████████████████████████| 40 kB 6.0 MB/s             \n",
            "\u001b[?25hCollecting google-apitools<1,>=0.5\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "     |████████████████████████████████| 135 kB 62.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx) (7.1.2)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.42.0)\n",
            "Collecting tfx-bsl<1.5.0,>=1.4.0\n",
            "  Downloading tfx_bsl-1.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.1 MB)\n",
            "     |████████████████████████████████| 19.1 MB 533 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.12.8)\n",
            "Collecting tensorflow-transform<1.5.0,>=1.4.0\n",
            "  Downloading tensorflow_transform-1.4.0-py3-none-any.whl (413 kB)\n",
            "     |████████████████████████████████| 413 kB 62.1 MB/s            \n",
            "\u001b[?25hCollecting attrs<21,>=19.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "     |████████████████████████████████| 49 kB 5.7 MB/s             \n",
            "\u001b[?25hCollecting google-cloud-bigquery<3,>=2.26.0\n",
            "  Downloading google_cloud_bigquery-2.30.1-py2.py3-none-any.whl (203 kB)\n",
            "     |████████████████████████████████| 203 kB 75.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.11.3)\n",
            "Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.19.5)\n",
            "Collecting ml-metadata<1.5.0,>=1.4.0\n",
            "  Downloading ml_metadata-1.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "     |████████████████████████████████| 6.6 MB 51.2 MB/s            \n",
            "\u001b[?25hCollecting docker<5,>=4.1\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "     |████████████████████████████████| 147 kB 53.4 MB/s            \n",
            "\u001b[?25hCollecting tensorflow-data-validation<1.5.0,>=1.4.0\n",
            "  Downloading tensorflow_data_validation-1.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 59.2 MB/s            \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.36,>=0.35.0\n",
            "  Downloading tensorflow_model_analysis-0.35.0-py3-none-any.whl (1.8 MB)\n",
            "     |████████████████████████████████| 1.8 MB 43.6 MB/s            \n",
            "\u001b[?25hCollecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2\n",
            "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "     |████████████████████████████████| 458.3 MB 12 kB/s              \n",
            "\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.3.9)\n",
            "Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.13)\n",
            "Collecting apache-beam[gcp]<3,>=2.33\n",
            "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "     |████████████████████████████████| 9.8 MB 41.5 MB/s            \n",
            "\u001b[?25hCollecting keras-tuner<2,>=1.0.4\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "     |████████████████████████████████| 98 kB 7.3 MB/s             \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting google-cloud-aiplatform<2,>=1.5.0\n",
            "  Downloading google_cloud_aiplatform-1.7.1-py2.py3-none-any.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 12.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n",
            "Requirement already satisfied: pyarrow<6,>=1 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.0.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.17.3)\n",
            "Collecting kubernetes<13,>=10.0.1\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "     |████████████████████████████████| 1.7 MB 23.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<0.13,>=0.9->tfx) (1.15.0)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "     |████████████████████████████████| 62 kB 794 kB/s             \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (3.10.0.2)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "     |████████████████████████████████| 2.3 MB 44.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (3.12.1)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "     |████████████████████████████████| 249 kB 60.8 MB/s            \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "     |████████████████████████████████| 151 kB 60.9 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (2.8.2)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (1.3.0)\n",
            "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (0.17.4)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (2018.9)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "     |████████████████████████████████| 829 kB 43.2 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (4.1.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (1.7)\n",
            "Collecting google-cloud-dlp<2,>=0.12.0\n",
            "  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
            "     |████████████████████████████████| 169 kB 58.2 MB/s            \n",
            "\u001b[?25hCollecting google-apitools<1,>=0.5\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "     |████████████████████████████████| 173 kB 60.5 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
            "     |████████████████████████████████| 83 kB 1.9 MB/s             \n",
            "\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0\n",
            "  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
            "     |████████████████████████████████| 144 kB 52.7 MB/s            \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
            "     |████████████████████████████████| 435 kB 64.0 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (4.2.4)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
            "     |████████████████████████████████| 267 kB 54.9 MB/s            \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
            "     |████████████████████████████████| 183 kB 61.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (1.0.3)\n",
            "Collecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
            "     |████████████████████████████████| 180 kB 73.1 MB/s            \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
            "     |████████████████████████████████| 255 kB 72.3 MB/s            \n",
            "\u001b[?25hCollecting google-cloud-bigquery-storage>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.10.1-py2.py3-none-any.whl (171 kB)\n",
            "     |████████████████████████████████| 171 kB 62.0 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.33->tfx) (1.8.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "     |████████████████████████████████| 52 kB 1.4 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (0.0.4)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting proto-plus>=1.10.1\n",
            "  Downloading proto_plus-1.19.8-py3-none-any.whl (45 kB)\n",
            "     |████████████████████████████████| 45 kB 3.0 MB/s             \n",
            "\u001b[?25hCollecting google-cloud-storage<2.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
            "     |████████████████████████████████| 106 kB 61.7 MB/s            \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-2.1.0-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 4.3 MB/s             \n",
            "\u001b[?25hCollecting google-api-core[grpc]<3.0.0dev,>=1.26.0\n",
            "  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n",
            "     |████████████████████████████████| 95 kB 2.5 MB/s             \n",
            "\u001b[?25hCollecting google-cloud-core<2,>=0.28.1\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.0.1)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (2.7.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (57.4.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2021.10.8)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.0)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<21,>=20->tfx) (3.0.6)\n",
            "Collecting typing-extensions<4,>=3.7.0\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (0.37.0)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting keras<2.7,>=2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "     |████████████████████████████████| 1.3 MB 62.7 MB/s            \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.7,>=2.6.0\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "     |████████████████████████████████| 462 kB 59.9 MB/s            \n",
            "\u001b[?25hCollecting tensorboard\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "     |████████████████████████████████| 5.6 MB 58.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (1.1.2)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (3.3.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (3.1.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (0.2.0)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<1.5.0,>=1.4.0->tfx) (1.1.5)\n",
            "Collecting joblib<0.15,>=0.12\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
            "     |████████████████████████████████| 294 kB 59.3 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<1.5,>=1.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<1.5.0,>=1.4.0->tfx) (1.4.0)\n",
            "Collecting ipython\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "     |████████████████████████████████| 790 kB 52.9 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.36,>=0.35.0->tfx) (7.6.5)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.6.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.8->tfx) (1.53.0)\n",
            "Collecting google-api-core[grpc]<3.0.0dev,>=1.26.0\n",
            "  Downloading google_api_core-2.2.1-py2.py3-none-any.whl (95 kB)\n",
            "     |████████████████████████████████| 95 kB 4.5 MB/s             \n",
            "\u001b[?25h  Downloading google_api_core-2.2.0-py2.py3-none-any.whl (95 kB)\n",
            "     |████████████████████████████████| 95 kB 3.8 MB/s             \n",
            "\u001b[?25h  Downloading google_api_core-2.1.1-py2.py3-none-any.whl (95 kB)\n",
            "     |████████████████████████████████| 95 kB 3.9 MB/s             \n",
            "\u001b[?25h  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
            "     |████████████████████████████████| 94 kB 3.2 MB/s             \n",
            "\u001b[?25h  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
            "     |████████████████████████████████| 92 kB 456 kB/s             \n",
            "\u001b[?25h  Downloading google_api_core-2.0.0-py2.py3-none-any.whl (92 kB)\n",
            "     |████████████████████████████████| 92 kB 423 kB/s             \n",
            "\u001b[?25h  Downloading google_api_core-1.31.4-py2.py3-none-any.whl (93 kB)\n",
            "     |████████████████████████████████| 93 kB 1.6 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.33->tfx) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.33->tfx) (0.2.8)\n",
            "Collecting libcst>=0.2.5\n",
            "  Downloading libcst-0.3.23-py3-none-any.whl (517 kB)\n",
            "     |████████████████████████████████| 517 kB 76.7 MB/s            \n",
            "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2->tfx) (1.5.2)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.33->tfx) (0.6.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "     |████████████████████████████████| 374 kB 70.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (3.5.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.33->tfx) (0.4.8)\n",
            "Collecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 72.8 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tfx) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.33->tfx) (2.0.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython->keras-tuner<2,>=1.0.4->tfx) (0.8.2)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting pyyaml<6,>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "     |████████████████████████████████| 636 kB 46.9 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner<2,>=1.0.4->tfx) (4.8.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython->keras-tuner<2,>=1.0.4->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (5.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner<2,>=1.0.4->tfx) (3.6.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (22.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.36,>=0.35.0->tfx) (0.5.1)\n",
            "Building wheels for collected packages: google-apitools, avro-python3, clang, dill, future, wrapt, grpc-google-iam-v1\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=0da57ada0026d873cbf851cba2767f6459181e530bca3877a31d0c1c5fca22ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=f2da85718ee97fb0c791f8c3c718501756f2b893a71a9912e4f6df039672690a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=a3ebb30391854a5192830ab4d02ab6e4d1c4d3ce4be4356d235f2e3c75f21404\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=925281df6653640aea5118929f5a9d9fb14a20c672d471a55774d2ddc5db68da\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=28485055fa2ee3151dee2ff119cc26a8797e8e9a52c4304ff3761ca57f08639d\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68719 sha256=a805a954191521bbff22531d603b16194121602a8a494f3959b14dd0b6adda00\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=3af486e1d55ced8a0a18a884439ff0432ae732efcd940a32da6630ca72100eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
            "Successfully built google-apitools avro-python3 clang dill future wrapt grpc-google-iam-v1\n",
            "Installing collected packages: typing-extensions, requests, protobuf, prompt-toolkit, packaging, mypy-extensions, typing-inspect, pyyaml, ipython, grpcio-gcp, google-crc32c, google-api-core, wrapt, tensorflow-estimator, tensorboard, proto-plus, orjson, libcst, keras, hdfs, grpc-google-iam-v1, google-resumable-media, google-cloud-core, future, flatbuffers, fasteners, fastavro, dill, clang, avro-python3, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, google-apitools, apache-beam, websocket-client, tensorflow-serving-api, attrs, tfx-bsl, ml-metadata, kt-legacy, joblib, google-cloud-storage, docker, tensorflow-transform, tensorflow-model-analysis, tensorflow-data-validation, ml-pipelines-sdk, kubernetes, keras-tuner, google-cloud-aiplatform, tfx\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: google-cloud-bigquery-storage\n",
            "    Found existing installation: google-cloud-bigquery-storage 1.1.0\n",
            "    Uninstalling google-cloud-bigquery-storage-1.1.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-storage-1.1.0\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.30.1 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.34.0 attrs-20.3.0 avro-python3-1.9.2.1 clang-5.0 dill-0.3.1.1 docker-4.4.4 fastavro-1.4.7 fasteners-0.16.3 flatbuffers-1.12 future-0.18.2 google-api-core-1.31.4 google-apitools-0.5.31 google-cloud-aiplatform-1.7.1 google-cloud-bigquery-2.30.1 google-cloud-bigquery-storage-2.10.1 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-1.19.1 google-cloud-storage-1.43.0 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 google-crc32c-1.3.0 google-resumable-media-2.1.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 ipython-7.29.0 joblib-0.14.1 keras-2.6.0 keras-tuner-1.1.0 kt-legacy-1.0.4 kubernetes-12.0.1 libcst-0.3.23 ml-metadata-1.4.0 ml-pipelines-sdk-1.4.0 mypy-extensions-0.4.3 orjson-3.6.4 packaging-20.9 prompt-toolkit-3.0.22 proto-plus-1.19.8 protobuf-3.19.1 pyyaml-5.4.1 requests-2.26.0 tensorboard-2.6.0 tensorflow-2.6.2 tensorflow-data-validation-1.4.0 tensorflow-estimator-2.6.0 tensorflow-model-analysis-0.35.0 tensorflow-serving-api-2.6.2 tensorflow-transform-1.4.0 tfx-1.4.0 tfx-bsl-1.4.0 typing-extensions-3.7.4.3 typing-inspect-0.7.1 websocket-client-1.2.1 wrapt-1.12.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "google",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwT0nov5QO1M"
      },
      "source": [
        "### Did you restart the runtime?\n",
        "\n",
        "If you are using Google Colab, the first time that you run\n",
        "the cell above, you must restart the runtime by clicking\n",
        "above \"RESTART RUNTIME\" button or using \"Runtime > Restart\n",
        "runtime ...\" menu. This is because of the way that Colab\n",
        "loads packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnPgN8UJtzN"
      },
      "source": [
        "Check the TensorFlow and TFX versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jh7vKSRqPHb",
        "outputId": "e9c75294-21b5-4666-f575-95a1dd1bdb64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.6.2\n",
            "TFX version: 1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "There are some variables used to define a pipeline. You can customize these\n",
        "variables as you want. By default all output from the pipeline will be\n",
        "generated under the current directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_NAME = \"penguin-transform\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Set default logging level."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsO0l5F3dzOr"
      },
      "source": [
        "### Prepare example data\n",
        "We will download the example dataset for use in our TFX pipeline. The dataset\n",
        "we are using is\n",
        "[Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html).\n",
        "\n",
        "However, unlike previous tutorials which used an already preprocessed dataset,\n",
        "we will use the **raw** Palmer Penguins dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11J7XiCq6AFP"
      },
      "source": [
        "Because the TFX ExampleGen component reads inputs from a directory, we need\n",
        "to create a directory and copy the dataset to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxMs6u86acP",
        "outputId": "196c8cd3-cd7a-42a3-d823-c93d01204259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
        "_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_path, _data_filepath)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/tmp/tfx-data1iu1ig66/data.csv', <http.client.HTTPMessage at 0x7f2658972150>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpoNmxKSQjI"
      },
      "source": [
        "Take a quick look at what the raw data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eSz28UDSnlG",
        "outputId": "513061d1-6378-4214-e31b-ba2f11f5a3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head {_data_filepath}\n",
        "print({_data_filepath})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "species,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\n",
            "Adelie,Torgersen,39.1,18.7,181,3750,MALE\n",
            "Adelie,Torgersen,39.5,17.4,186,3800,FEMALE\n",
            "Adelie,Torgersen,40.3,18,195,3250,FEMALE\n",
            "Adelie,Torgersen,36.7,19.3,193,3450,FEMALE\n",
            "Adelie,Torgersen,39.3,20.6,190,3650,MALE\n",
            "Adelie,Torgersen,38.9,17.8,181,3625,FEMALE\n",
            "Adelie,Torgersen,39.2,19.6,195,4675,MALE\n",
            "Adelie,Torgersen,41.1,17.6,182,3200,FEMALE\n",
            "Adelie,Torgersen,38.6,21.2,191,3800,MALE\n",
            "{'/tmp/tfx-data1iu1ig66/data.csv'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTtQNq1DdVvG"
      },
      "source": [
        "There are some entries with missing values which are represented as `NA`.\n",
        "We will just delete those entries in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQhpoaqff9ca",
        "outputId": "f2f02772-332f-4f60-e397-ae047cad01b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sed -i '/\\bNA\\b/d' {_data_filepath}\n",
        "!head {_data_filepath}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "species,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\n",
            "Adelie,Torgersen,39.1,18.7,181,3750,MALE\n",
            "Adelie,Torgersen,39.5,17.4,186,3800,FEMALE\n",
            "Adelie,Torgersen,40.3,18,195,3250,FEMALE\n",
            "Adelie,Torgersen,36.7,19.3,193,3450,FEMALE\n",
            "Adelie,Torgersen,39.3,20.6,190,3650,MALE\n",
            "Adelie,Torgersen,38.9,17.8,181,3625,FEMALE\n",
            "Adelie,Torgersen,39.2,19.6,195,4675,MALE\n",
            "Adelie,Torgersen,41.1,17.6,182,3200,FEMALE\n",
            "Adelie,Torgersen,38.6,21.2,191,3800,MALE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8EOfCy1dzO2"
      },
      "source": [
        "You should be able to see seven features which describe penguins. We will use\n",
        "the same set of features as the previous tutorials - 'culmen_length_mm',\n",
        "'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' - and will predict\n",
        "the 'species' of a penguin.\n",
        "\n",
        "**The only difference will be that the input data is not preprocessed.** Note\n",
        "that we will not use other features like 'island' or 'sex' in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtbrkjjc-IKA"
      },
      "source": [
        "### Prepare a schema file\n",
        "\n",
        "As described in\n",
        "[Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv),\n",
        "we need a schema file for the dataset. Because the dataset is different from the previous tutorial we need to generate it again. In this tutorial, we will skip those steps and just use a prepared schema file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoB97m8B9nG",
        "outputId": "44128772-07c2-4c81-dcf9-e4e4abc03f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import shutil\n",
        "\n",
        "SCHEMA_PATH = 'schema'\n",
        "\n",
        "_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n",
        "_schema_filename = 'schema.pbtxt'\n",
        "_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n",
        "\n",
        "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
        "urllib.request.urlretrieve(_schema_uri, _schema_filepath)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('schema/schema.pbtxt', <http.client.HTTPMessage at 0x7f2658830b10>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKJ_HDJQB94b"
      },
      "source": [
        "This schema file was created with the same pipeline as in the previous tutorial\n",
        "without any manual changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## Create a pipeline\n",
        "\n",
        "TFX pipelines are defined using Python APIs. We will add `Transform`\n",
        "component to the pipeline we created in the\n",
        "[Data Validation tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv).\n",
        "\n",
        "A Transform component requires input data from an `ExampleGen` component and\n",
        "a schema from a `SchemaGen` component, and produces a \"transform graph\". The\n",
        "output will be used in a `Trainer` component. Transform can optionally\n",
        "produce \"transformed data\" in addition, which is the materialized data after\n",
        "transformation.\n",
        "However, we will transform data during training in this tutorial without\n",
        "materialization of the intermediate transformed data.\n",
        "\n",
        "One thing to note is that we need to define a Python function,\n",
        "`preprocessing_fn` to describe how input data should be transformed. This is\n",
        "similar to a Trainer component which also requires user code for model\n",
        "definition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### Write preprocessing and training code\n",
        "\n",
        "We need to define two Python functions. One for Transform and one for Trainer.\n",
        "\n",
        "#### preprocessing_fn\n",
        "The Transform component will find a function named `preprocessing_fn` in the\n",
        "given module file as we did for `Trainer` component. You can also specify a\n",
        "specific function using the\n",
        "[`preprocessing_fn` parameter](https://github.com/tensorflow/tfx/blob/142de6e887f26f4101ded7925f60d7d4fe9d42ed/tfx/components/transform/component.py#L113)\n",
        "of the Transform component.\n",
        "\n",
        "In this example, we will do two kinds of transformation. For continuous numeric\n",
        "features like `culmen_length_mm` and `body_mass_g`, we will normalize these\n",
        "values using the\n",
        "[tft.scale_to_z_score](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score)\n",
        "function. For the label feature, we need to convert string labels into numeric\n",
        "index values. We will use\n",
        "[`tf.lookup.StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable)\n",
        "for conversion.\n",
        "\n",
        "To identify transformed fields easily, we append a `_xf` suffix to the\n",
        "transformed feature names.\n",
        "\n",
        "#### run_fn\n",
        "\n",
        "The model itself is almost the same as in the previous tutorials, but this time\n",
        "we will transform the input data using the transform graph from the Transform\n",
        "component.\n",
        "\n",
        "One more important difference compared to the previous tutorial is that now we\n",
        "export a model for serving which includes not only the computation graph of the\n",
        "model, but also the transform graph for preprocessing, which is generated in\n",
        "Transform component. We need to define a separate function which will be used\n",
        "for serving incoming requests. You can see that the same function\n",
        "`_apply_preprocessing` was used for both of the training data and the\n",
        "serving request.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "source": [
        "_module_file = 'penguin_utils.py'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnc67uQNTDfW",
        "outputId": "93442b3e-5c20-49dc-f606-fadbf7c3673f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile {_module_file}\n",
        "\n",
        "\n",
        "from typing import List, Text\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "# Specify features that we will use.\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "\n",
        "# NEW: TFX Transform will call this function.\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "\n",
        "  # Uses features defined in _FEATURE_KEYS only.\n",
        "  for key in _FEATURE_KEYS:\n",
        "    # tft.scale_to_z_score computes the mean and variance of the given feature\n",
        "    # and scales the output based on the result.\n",
        "    outputs[key] = tft.scale_to_z_score(inputs[key])\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n",
        "  # compute the vocabulary dynamically and perform a lookup.\n",
        "  # Since in this example there are only 3 possible values, we use a hard-coded\n",
        "  # table for simplicity.\n",
        "  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n",
        "  initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      keys=table_keys,\n",
        "      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
        "      key_dtype=tf.string,\n",
        "      value_dtype=tf.int64)\n",
        "  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
        "  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "# NEW: This function will apply the same transform operation to training data\n",
        "#      and serving requests.\n",
        "def _apply_preprocessing(raw_features, tft_layer):\n",
        "  transformed_features = tft_layer(raw_features)\n",
        "  if _LABEL_KEY in raw_features:\n",
        "    transformed_label = transformed_features.pop(_LABEL_KEY)\n",
        "    return transformed_features, transformed_label\n",
        "  else:\n",
        "    return transformed_features, None\n",
        "\n",
        "\n",
        "# NEW: This function will create a handler function which gets a serialized\n",
        "#      tf.example, preprocess and run an inference with it.\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "  # We must save the tft_layer to the model to ensure its assets are kept and\n",
        "  # tracked.\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "  ])\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "    # Expected input is a string which is serialized tf.Example format.\n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    # Because input schema includes unnecessary fields like 'species' and\n",
        "    # 'island', we filter feature_spec to include required keys only.\n",
        "    required_feature_spec = {\n",
        "        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n",
        "    }\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
        "                                          required_feature_spec)\n",
        "\n",
        "    # Preprocess parsed input with transform operation defined in\n",
        "    # preprocessing_fn().\n",
        "    transformed_features, _ = _apply_preprocessing(parsed_features,\n",
        "                                                   model.tft_layer)\n",
        "    # Run inference with ML model.\n",
        "    return model(transformed_features)\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  dataset = data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n",
        "      schema=tf_transform_output.raw_metadata.schema)\n",
        "\n",
        "  transform_layer = tf_transform_output.transform_features_layer()\n",
        "  def apply_transform(raw_features):\n",
        "    return _apply_preprocessing(raw_features, transform_layer)\n",
        "\n",
        "  return dataset.map(apply_transform).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [\n",
        "      keras.layers.Input(shape=(1,), name=key)\n",
        "      for key in _FEATURE_KEYS\n",
        "  ]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      tf_transform_output,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      tf_transform_output,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # NEW: Save a computation graph including transform layer.\n",
        "  signatures = {\n",
        "      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
        "  }\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing penguin_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaw0rs-emEf"
      },
      "source": [
        "Now you have completed all of the preparation steps to build a TFX pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### Write a pipeline definition\n",
        "\n",
        "We define a function to create a TFX pipeline. A `Pipeline` object\n",
        "represents a TFX pipeline, which can be run using one of the pipeline\n",
        "orchestration systems that TFX supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "source": [
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     schema_path: str, module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n",
        "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # Import the schema.\n",
        "  schema_importer = tfx.dsl.Importer(\n",
        "      source_uri=schema_path,\n",
        "      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
        "          'schema_importer')\n",
        "\n",
        "  # Performs anomaly detection based on statistics and data schema.\n",
        "  example_validator = tfx.components.ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_importer.outputs['result'])\n",
        "\n",
        "  # NEW: Transforms input data using preprocessing_fn in the 'module_file'.\n",
        "  transform = tfx.components.Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_importer.outputs['result'],\n",
        "      materialize=False,\n",
        "      module_file=module_file)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "\n",
        "      # NEW: Pass transform_graph to the trainer.\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a filesystem destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "      statistics_gen,\n",
        "      schema_importer,\n",
        "      example_validator,\n",
        "\n",
        "      transform,  # NEW: Transform component was added to the pipeline.\n",
        "\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbq07THU2GV"
      },
      "source": [
        "## Run the pipeline\n",
        "\n",
        "We will use `LocalDagRunner` as in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAtfOZTYWJu-"
      },
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      schema_path=SCHEMA_PATH,\n",
        "      module_file=_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppERq0Mj6xvW"
      },
      "source": [
        "You should see \"INFO:absl:Component Pusher is finished.\" if the pipeline\n",
        "finished successfully.\n",
        "\n",
        "The pusher component pushes the trained model to the `SERVING_MODEL_DIR` which\n",
        "is the `serving_model/penguin-transform` directory if you did not change\n",
        "the variables in the previous steps. You can see the result from the file\n",
        "browser in the left-side panel in Colab, or using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHROkqX6yHx",
        "outputId": "4d1a4b67-48c1-4157-f919-9aa131a48079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List files in created model directory.\n",
        "!find {SERVING_MODEL_DIR}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_model/penguin-transform\n",
            "serving_model/penguin-transform/1637836332\n",
            "serving_model/penguin-transform/1637836332/assets\n",
            "serving_model/penguin-transform/1637836332/keras_metadata.pb\n",
            "serving_model/penguin-transform/1637836332/saved_model.pb\n",
            "serving_model/penguin-transform/1637836332/variables\n",
            "serving_model/penguin-transform/1637836332/variables/variables.data-00000-of-00001\n",
            "serving_model/penguin-transform/1637836332/variables/variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTqM-WiZkPbt"
      },
      "source": [
        "You can also check the signature of the generated model using the\n",
        "[`saved_model_cli` tool](https://www.tensorflow.org/guide/saved_model#show_command)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBfUzD_OkOq_",
        "outputId": "8d32995e-a44e-4b59-e84b-9f5364d3b325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {SERVING_MODEL_DIR}/$(ls -1 {SERVING_MODEL_DIR} | sort -nr | head -1) --tag_set serve --signature_def serving_default"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['examples'] tensor_info:\n",
            "      dtype: DT_STRING\n",
            "      shape: (-1)\n",
            "      name: serving_default_examples:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['output_0'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 3)\n",
            "      name: StatefulPartitionedCall_2:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkAxFs_QszoZ"
      },
      "source": [
        "Because we defined `serving_default` with our own `serve_tf_examples_fn`\n",
        "function, the signature shows that it takes a single string.\n",
        "This string is a serialized string of tf.Examples and will be parsed with the\n",
        "[tf.io.parse_example()](https://www.tensorflow.org/api_docs/python/tf/io/parse_example)\n",
        "function as we defined earlier (learn more about tf.Examples [here](https://www.tensorflow.org/tutorials/load_data/tfrecord)).\n",
        "\n",
        "We can load the exported model and try some inferences with a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Yw5yYdvqKf",
        "outputId": "be32aeba-0b61-4d6b-cf6c-6032f7ef6875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Find a model with the latest timestamp.\n",
        "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
        "model_path = max(model_dirs, key=lambda i: int(i.name)).path\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "inference_fn = loaded_model.signatures['serving_default']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f2651737090> and <keras.engine.input_layer.InputLayer object at 0x7f26538017d0>).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f2651737090> and <keras.engine.input_layer.InputLayer object at 0x7f26538017d0>).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrOHIvnIv0-4",
        "outputId": "56725b06-0c9d-4c80-e240-f2bd022254e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prepare an example and run inference.\n",
        "features = {\n",
        "  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.9])),\n",
        "  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.1])),\n",
        "  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[213])),\n",
        "  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[5400])),\n",
        "}\n",
        "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "examples = example_proto.SerializeToString()\n",
        "\n",
        "result = inference_fn(examples=tf.constant([examples]))\n",
        "print(result['output_0'].numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.1969142 -2.5477448  5.6736374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cri3mTgZ0SQ2"
      },
      "source": [
        "The third element, which corresponds to 'Gentoo' species, is expected to be the\n",
        "largest among three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08R8qvweThRf"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "If you want to learn more about Transform component, see\n",
        "[Transform Component guide](https://www.tensorflow.org/tfx/guide/transform).\n",
        "You can find more resources on https://www.tensorflow.org/tfx/tutorials.\n",
        "\n",
        "Please see\n",
        "[Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines)\n",
        "to learn more about various concepts in TFX.\n"
      ]
    }
  ]
}